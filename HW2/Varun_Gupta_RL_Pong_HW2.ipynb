{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#SELECT GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    #Change reshape to return flattened vector\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create  game environment and examine action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\") #choose the game\n",
    "\n",
    "obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 0, reward_sum -21.00, number of steps 1347, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 50, reward_sum -20.00, number of steps 1380, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 100, reward_sum -20.00, number of steps 1229, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 150, reward_sum -21.00, number of steps 1415, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 200, reward_sum -20.00, number of steps 1470, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 250, reward_sum -21.00, number of steps 1663, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 300, reward_sum -20.00, number of steps 1322, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 350, reward_sum -20.00, number of steps 2033, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -19.0\n",
      "episode 400, reward_sum -19.00, number of steps 1678, number of wins 2, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 450, reward_sum -21.00, number of steps 1747, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 500, reward_sum -20.00, number of steps 1973, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 550, reward_sum -20.00, number of steps 2033, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 600, reward_sum -20.00, number of steps 1612, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 650, reward_sum -20.00, number of steps 1969, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 700, reward_sum -21.00, number of steps 1813, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 750, reward_sum -17.00, number of steps 2794, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -19.0\n",
      "episode 800, reward_sum -19.00, number of steps 2312, number of wins 2, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -19.0\n",
      "episode 850, reward_sum -19.00, number of steps 2012, number of wins 2, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 900, reward_sum -21.00, number of steps 2006, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -18.0\n",
      "episode 950, reward_sum -18.00, number of steps 2135, number of wins 3, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1000, reward_sum -15.00, number of steps 2349, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1050, reward_sum -15.00, number of steps 2430, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 1100, reward_sum -17.00, number of steps 2549, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1150, reward_sum -15.00, number of steps 3891, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 1200, reward_sum -21.00, number of steps 2790, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1250, reward_sum -16.00, number of steps 2926, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 1300, reward_sum -17.00, number of steps 3215, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 1350, reward_sum -12.00, number of steps 3557, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1400, reward_sum -15.00, number of steps 2417, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 1450, reward_sum -17.00, number of steps 3209, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 1500, reward_sum -12.00, number of steps 3344, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1550, reward_sum -16.00, number of steps 3461, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -19.0\n",
      "episode 1600, reward_sum -19.00, number of steps 2824, number of wins 2, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1650, reward_sum -16.00, number of steps 3450, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -18.0\n",
      "episode 1700, reward_sum -18.00, number of steps 3776, number of wins 3, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1750, reward_sum -16.00, number of steps 3095, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1800, reward_sum -16.00, number of steps 3194, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 1850, reward_sum -13.00, number of steps 4211, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 1900, reward_sum -13.00, number of steps 5121, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1950, reward_sum -15.00, number of steps 4375, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 2000, reward_sum -16.00, number of steps 4562, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 2050, reward_sum -10.00, number of steps 5168, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 2100, reward_sum -15.00, number of steps 3967, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 2150, reward_sum -17.00, number of steps 3315, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 2200, reward_sum -13.00, number of steps 4264, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2250, reward_sum -12.00, number of steps 5484, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 2300, reward_sum -10.00, number of steps 5535, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 2350, reward_sum -10.00, number of steps 6335, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 2400, reward_sum -11.00, number of steps 5775, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 2450, reward_sum -15.00, number of steps 4996, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2500, reward_sum -12.00, number of steps 6075, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2550, reward_sum -12.00, number of steps 4752, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 2600, reward_sum -11.00, number of steps 5716, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 2650, reward_sum -3.00, number of steps 5729, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 2700, reward_sum -15.00, number of steps 5125, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2750, reward_sum -12.00, number of steps 5368, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 2800, reward_sum 3.00, number of steps 6486, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 2850, reward_sum -11.00, number of steps 5888, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2900, reward_sum -12.00, number of steps 5161, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 2950, reward_sum -9.00, number of steps 6945, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 3000, reward_sum -16.00, number of steps 4814, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 3050, reward_sum -17.00, number of steps 3871, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 3100, reward_sum -16.00, number of steps 4078, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -18.0\n",
      "episode 3150, reward_sum -18.00, number of steps 4390, number of wins 3, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -14.0\n",
      "episode 3200, reward_sum -14.00, number of steps 5406, number of wins 7, number of losses 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 3250, reward_sum -17.00, number of steps 3547, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 3300, reward_sum -1.00, number of steps 7440, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 3350, reward_sum -13.00, number of steps 5929, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 3400, reward_sum -11.00, number of steps 5713, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 3450, reward_sum -7.00, number of steps 6867, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 3500, reward_sum -8.00, number of steps 6075, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 3550, reward_sum -12.00, number of steps 5073, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 3600, reward_sum -5.00, number of steps 7742, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 3650, reward_sum -9.00, number of steps 5802, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 3700, reward_sum -6.00, number of steps 7870, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 3750, reward_sum -9.00, number of steps 7165, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 3800, reward_sum -10.00, number of steps 6116, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 3850, reward_sum -4.00, number of steps 7831, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 3900, reward_sum -5.00, number of steps 6513, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 3950, reward_sum -10.00, number of steps 5501, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 4000, reward_sum -11.00, number of steps 5947, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 4050, reward_sum -3.00, number of steps 6720, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 4100, reward_sum 3.00, number of steps 7966, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 4150, reward_sum -13.00, number of steps 5282, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 4200, reward_sum -9.00, number of steps 6393, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 4250, reward_sum 15.00, number of steps 4331, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 4300, reward_sum -11.00, number of steps 5841, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 4350, reward_sum -6.00, number of steps 8777, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 4400, reward_sum -16.00, number of steps 6081, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 4450, reward_sum -2.00, number of steps 9568, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 4500, reward_sum -8.00, number of steps 7373, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 4550, reward_sum -11.00, number of steps 6457, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 4600, reward_sum -8.00, number of steps 5239, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 4650, reward_sum -1.00, number of steps 7190, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 4700, reward_sum -1.00, number of steps 8384, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 4750, reward_sum -1.00, number of steps 7439, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 4800, reward_sum 2.00, number of steps 6741, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 4850, reward_sum 1.00, number of steps 6408, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 4900, reward_sum -9.00, number of steps 8241, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 4950, reward_sum -7.00, number of steps 6732, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 5000, reward_sum -11.00, number of steps 6015, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 5050, reward_sum -11.00, number of steps 6236, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 5100, reward_sum 10.00, number of steps 5522, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 5150, reward_sum -3.00, number of steps 9162, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 5200, reward_sum -9.00, number of steps 7496, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -14.0\n",
      "episode 5250, reward_sum -14.00, number of steps 4639, number of wins 7, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 5300, reward_sum 9.00, number of steps 6111, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 5350, reward_sum -5.00, number of steps 7523, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 5400, reward_sum -12.00, number of steps 4908, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 5450, reward_sum -7.00, number of steps 6588, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 5500, reward_sum -17.00, number of steps 6060, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 5550, reward_sum 2.00, number of steps 8502, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 5600, reward_sum 8.00, number of steps 7315, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 5650, reward_sum -9.00, number of steps 5747, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 5700, reward_sum -3.00, number of steps 7645, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 5750, reward_sum -5.00, number of steps 8170, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 5800, reward_sum -7.00, number of steps 7897, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 5850, reward_sum -2.00, number of steps 7076, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 5900, reward_sum 6.00, number of steps 7080, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 5950, reward_sum -1.00, number of steps 7996, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 6000, reward_sum -2.00, number of steps 8124, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 6050, reward_sum -6.00, number of steps 7369, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -14.0\n",
      "episode 6100, reward_sum -14.00, number of steps 5977, number of wins 7, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 6150, reward_sum -5.00, number of steps 6986, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 6200, reward_sum 9.00, number of steps 5331, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 6250, reward_sum 8.00, number of steps 6737, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 6300, reward_sum 12.00, number of steps 4682, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 6350, reward_sum 7.00, number of steps 7006, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 6400, reward_sum 7.00, number of steps 7191, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 6450, reward_sum 11.00, number of steps 5578, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 6500, reward_sum 8.00, number of steps 7068, number of wins 21, number of losses 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 6550, reward_sum -9.00, number of steps 6257, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 6600, reward_sum -1.00, number of steps 8916, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 6650, reward_sum -5.00, number of steps 6785, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 6700, reward_sum -5.00, number of steps 7898, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 6750, reward_sum -9.00, number of steps 8576, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 6800, reward_sum 3.00, number of steps 7639, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 6850, reward_sum 8.00, number of steps 7114, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 6900, reward_sum -4.00, number of steps 7928, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 6950, reward_sum 7.00, number of steps 7663, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 7000, reward_sum 2.00, number of steps 9029, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 7050, reward_sum 1.00, number of steps 9472, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 7100, reward_sum -5.00, number of steps 8149, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 7150, reward_sum 9.00, number of steps 7127, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 7200, reward_sum -3.00, number of steps 8198, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 7250, reward_sum 10.00, number of steps 4708, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 7300, reward_sum -5.00, number of steps 7008, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 7350, reward_sum 6.00, number of steps 9623, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 7400, reward_sum -12.00, number of steps 5897, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 7450, reward_sum -8.00, number of steps 7432, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 7500, reward_sum 4.00, number of steps 9085, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 7550, reward_sum -1.00, number of steps 9224, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 7600, reward_sum -3.00, number of steps 10000, number of wins 17, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 7650, reward_sum -5.00, number of steps 7211, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 7700, reward_sum -7.00, number of steps 6920, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 7750, reward_sum -2.00, number of steps 9827, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 7800, reward_sum -3.00, number of steps 9158, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 7850, reward_sum 11.00, number of steps 6952, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 7900, reward_sum 2.00, number of steps 9174, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 7950, reward_sum 6.00, number of steps 7866, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 8000, reward_sum 8.00, number of steps 6021, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 8050, reward_sum 5.00, number of steps 8437, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 8100, reward_sum 9.00, number of steps 6435, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 8150, reward_sum -2.00, number of steps 8920, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 8200, reward_sum -2.00, number of steps 8972, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 8250, reward_sum 8.00, number of steps 7304, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 8300, reward_sum 6.00, number of steps 6585, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 8350, reward_sum 7.00, number of steps 6297, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 8400, reward_sum 6.00, number of steps 7725, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 8450, reward_sum 4.00, number of steps 7328, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 8500, reward_sum 5.00, number of steps 7758, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 8550, reward_sum 15.00, number of steps 5869, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 8600, reward_sum 7.00, number of steps 8751, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 8650, reward_sum 3.00, number of steps 7721, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 8700, reward_sum -3.00, number of steps 6158, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 8750, reward_sum -6.00, number of steps 7954, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 8800, reward_sum -1.00, number of steps 8165, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 8850, reward_sum 4.00, number of steps 7599, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 8900, reward_sum -8.00, number of steps 8681, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 8950, reward_sum -3.00, number of steps 10000, number of wins 15, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 9000, reward_sum 3.00, number of steps 8164, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 9050, reward_sum 7.00, number of steps 7790, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 9100, reward_sum 2.00, number of steps 8759, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 9150, reward_sum -4.00, number of steps 7476, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 9200, reward_sum -7.00, number of steps 9188, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 9250, reward_sum 4.00, number of steps 7572, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 9300, reward_sum -4.00, number of steps 9021, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 9350, reward_sum 14.00, number of steps 3870, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 9400, reward_sum 5.00, number of steps 7145, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 9450, reward_sum 7.00, number of steps 7589, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 9500, reward_sum 6.00, number of steps 6996, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 9550, reward_sum 1.00, number of steps 8183, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 9600, reward_sum -2.00, number of steps 8423, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 9650, reward_sum -6.00, number of steps 8449, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 9700, reward_sum 15.00, number of steps 6024, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 9750, reward_sum 2.00, number of steps 8247, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 9800, reward_sum 4.00, number of steps 7113, number of wins 21, number of losses 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 9850, reward_sum 13.00, number of steps 6241, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 9900, reward_sum 1.00, number of steps 7749, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 16.0\n",
      "episode 9950, reward_sum 16.00, number of steps 4607, number of wins 21, number of losses 5\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 10000, reward_sum 1.00, number of steps 8058, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 10050, reward_sum 11.00, number of steps 5392, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 10100, reward_sum 6.00, number of steps 8678, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 18.0\n",
      "episode 10150, reward_sum 18.00, number of steps 4120, number of wins 21, number of losses 3\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 10200, reward_sum 7.00, number of steps 9081, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 10250, reward_sum 7.00, number of steps 8057, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 10300, reward_sum -4.00, number of steps 8398, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 10350, reward_sum 4.00, number of steps 7569, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 10400, reward_sum 9.00, number of steps 6154, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 10450, reward_sum 10.00, number of steps 6078, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 10500, reward_sum 1.00, number of steps 9089, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 10550, reward_sum -12.00, number of steps 7659, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 10600, reward_sum 6.00, number of steps 8430, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 10650, reward_sum 5.00, number of steps 7477, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 10700, reward_sum 10.00, number of steps 6419, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 10750, reward_sum 8.00, number of steps 6349, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 10800, reward_sum 13.00, number of steps 6802, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 10850, reward_sum -7.00, number of steps 9147, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 10900, reward_sum -10.00, number of steps 6642, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 10950, reward_sum -1.00, number of steps 9308, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 11000, reward_sum 8.00, number of steps 5681, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 11050, reward_sum -8.00, number of steps 6785, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 11100, reward_sum -2.00, number of steps 8165, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 11150, reward_sum -7.00, number of steps 7883, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 11200, reward_sum 2.00, number of steps 7407, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 11250, reward_sum -2.00, number of steps 7569, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 11300, reward_sum 4.00, number of steps 8029, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 11350, reward_sum -1.00, number of steps 8538, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 11400, reward_sum 6.00, number of steps 7387, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 11450, reward_sum 2.00, number of steps 6221, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 11500, reward_sum 8.00, number of steps 7380, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 11550, reward_sum 3.00, number of steps 6908, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 11600, reward_sum 15.00, number of steps 5672, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 11650, reward_sum 5.00, number of steps 6815, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 11700, reward_sum 10.00, number of steps 5562, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 11750, reward_sum 1.00, number of steps 8630, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 11800, reward_sum 4.00, number of steps 8220, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 11850, reward_sum 2.00, number of steps 7616, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 11900, reward_sum 4.00, number of steps 6667, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 11950, reward_sum 5.00, number of steps 8197, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 12000, reward_sum 12.00, number of steps 6727, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 17.0\n",
      "episode 12050, reward_sum 17.00, number of steps 4124, number of wins 21, number of losses 4\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 12100, reward_sum -7.00, number of steps 7299, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 12150, reward_sum 1.00, number of steps 10000, number of wins 20, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 12200, reward_sum 4.00, number of steps 6963, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 12250, reward_sum 9.00, number of steps 7949, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 12300, reward_sum 13.00, number of steps 5689, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 12350, reward_sum -6.00, number of steps 7908, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 12400, reward_sum 6.00, number of steps 8548, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 12450, reward_sum -1.00, number of steps 8829, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 12500, reward_sum 5.00, number of steps 8500, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 12550, reward_sum 8.00, number of steps 7328, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 12600, reward_sum 9.00, number of steps 7540, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 12650, reward_sum 8.00, number of steps 6805, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 12700, reward_sum 2.00, number of steps 7573, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 12750, reward_sum 13.00, number of steps 5715, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 12800, reward_sum 12.00, number of steps 5860, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 12850, reward_sum 12.00, number of steps 6190, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 12900, reward_sum 10.00, number of steps 6137, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 12950, reward_sum 1.00, number of steps 8846, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 13000, reward_sum 10.00, number of steps 6179, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 13050, reward_sum 10.00, number of steps 5987, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 13100, reward_sum 2.00, number of steps 10000, number of wins 20, number of losses 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 13150, reward_sum 7.00, number of steps 7401, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 13200, reward_sum 8.00, number of steps 6689, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 13250, reward_sum 7.00, number of steps 7290, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 13300, reward_sum 8.00, number of steps 7279, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 13350, reward_sum 8.00, number of steps 8212, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 13400, reward_sum -2.00, number of steps 8327, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 16.0\n",
      "episode 13450, reward_sum 16.00, number of steps 7160, number of wins 21, number of losses 5\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 13500, reward_sum -4.00, number of steps 7741, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 13550, reward_sum 3.00, number of steps 7494, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 13600, reward_sum 4.00, number of steps 8736, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 13650, reward_sum 12.00, number of steps 5582, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 13700, reward_sum 6.00, number of steps 6761, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 13750, reward_sum -3.00, number of steps 8918, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 13800, reward_sum 7.00, number of steps 7252, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 13850, reward_sum 5.00, number of steps 8190, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 13900, reward_sum 6.00, number of steps 7169, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 13950, reward_sum 5.00, number of steps 6885, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 14000, reward_sum 10.00, number of steps 6723, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 14050, reward_sum 2.00, number of steps 9433, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 14100, reward_sum 5.00, number of steps 9177, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 14150, reward_sum 2.00, number of steps 9938, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 14200, reward_sum 11.00, number of steps 6468, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 14250, reward_sum -10.00, number of steps 7122, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 14300, reward_sum 9.00, number of steps 6221, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 14350, reward_sum 7.00, number of steps 7198, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 14400, reward_sum -1.00, number of steps 9167, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 14450, reward_sum 4.00, number of steps 7347, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 14500, reward_sum 8.00, number of steps 8575, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 14550, reward_sum 11.00, number of steps 5310, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 14600, reward_sum 11.00, number of steps 8701, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 14650, reward_sum 6.00, number of steps 10000, number of wins 17, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 14700, reward_sum 1.00, number of steps 7590, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 14750, reward_sum 3.00, number of steps 8901, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 14800, reward_sum 6.00, number of steps 7656, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 14850, reward_sum 11.00, number of steps 6273, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 14900, reward_sum 13.00, number of steps 6086, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 14950, reward_sum 5.00, number of steps 8768, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 15000, reward_sum 5.00, number of steps 6670, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 15050, reward_sum 4.00, number of steps 8211, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 15100, reward_sum -5.00, number of steps 9371, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 15150, reward_sum 2.00, number of steps 8332, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 15200, reward_sum 4.00, number of steps 8186, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 15250, reward_sum 6.00, number of steps 7546, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 15300, reward_sum 8.00, number of steps 6979, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 15350, reward_sum 3.00, number of steps 8281, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 15400, reward_sum 8.00, number of steps 6157, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 15450, reward_sum 11.00, number of steps 5556, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 15500, reward_sum 12.00, number of steps 6590, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 15550, reward_sum -2.00, number of steps 9545, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 15600, reward_sum 11.00, number of steps 7259, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 15650, reward_sum 5.00, number of steps 7648, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 15700, reward_sum 7.00, number of steps 7288, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 15750, reward_sum 12.00, number of steps 8683, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 15800, reward_sum 15.00, number of steps 4953, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 15850, reward_sum 11.00, number of steps 6540, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 15900, reward_sum 8.00, number of steps 7075, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 15950, reward_sum 6.00, number of steps 8148, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 16.0\n",
      "episode 16000, reward_sum 16.00, number of steps 4756, number of wins 21, number of losses 5\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 16050, reward_sum 1.00, number of steps 8565, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 16100, reward_sum 15.00, number of steps 5292, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 16150, reward_sum 9.00, number of steps 7480, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 16200, reward_sum 12.00, number of steps 5668, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 16250, reward_sum -1.00, number of steps 8074, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 16300, reward_sum 8.00, number of steps 7881, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 16350, reward_sum 3.00, number of steps 8926, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 16400, reward_sum 7.00, number of steps 7436, number of wins 21, number of losses 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 16450, reward_sum 2.00, number of steps 9087, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 17.0\n",
      "episode 16500, reward_sum 17.00, number of steps 3900, number of wins 21, number of losses 4\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 16550, reward_sum -5.00, number of steps 8432, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 16600, reward_sum 5.00, number of steps 7474, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 16650, reward_sum 5.00, number of steps 7046, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 16700, reward_sum 10.00, number of steps 5568, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 16750, reward_sum 2.00, number of steps 8469, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 16800, reward_sum 6.00, number of steps 8164, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 16850, reward_sum 10.00, number of steps 5094, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 16900, reward_sum 11.00, number of steps 6522, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 16950, reward_sum 13.00, number of steps 5024, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 17000, reward_sum 3.00, number of steps 7228, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 17050, reward_sum 4.00, number of steps 7164, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 17100, reward_sum 8.00, number of steps 6410, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 17150, reward_sum 4.00, number of steps 8245, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 17200, reward_sum 7.00, number of steps 7334, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 17250, reward_sum 2.00, number of steps 8236, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 17300, reward_sum -1.00, number of steps 8486, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 17350, reward_sum 11.00, number of steps 6318, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 17400, reward_sum 8.00, number of steps 7192, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 17450, reward_sum 9.00, number of steps 6704, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 17500, reward_sum 6.00, number of steps 8908, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 17550, reward_sum 10.00, number of steps 5970, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 17600, reward_sum 12.00, number of steps 6222, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 17650, reward_sum 7.00, number of steps 6304, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 17700, reward_sum 10.00, number of steps 5493, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 17750, reward_sum 10.00, number of steps 6312, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 17800, reward_sum 1.00, number of steps 7597, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 17850, reward_sum 10.00, number of steps 6134, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 17900, reward_sum 9.00, number of steps 7313, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 17950, reward_sum 11.00, number of steps 5462, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 18000, reward_sum -1.00, number of steps 9071, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 18050, reward_sum 12.00, number of steps 5369, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 18100, reward_sum 8.00, number of steps 8079, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 18150, reward_sum 9.00, number of steps 5562, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 18200, reward_sum 10.00, number of steps 7051, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 18250, reward_sum 13.00, number of steps 6027, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 18300, reward_sum 6.00, number of steps 7474, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 18350, reward_sum 14.00, number of steps 5496, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 18400, reward_sum 12.00, number of steps 6405, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 18450, reward_sum 15.00, number of steps 5803, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 18500, reward_sum -1.00, number of steps 7173, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 18550, reward_sum 14.00, number of steps 4486, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 18600, reward_sum 4.00, number of steps 7553, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 18650, reward_sum 2.00, number of steps 9342, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 18700, reward_sum 4.00, number of steps 7066, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 18750, reward_sum 9.00, number of steps 7007, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 16.0\n",
      "episode 18800, reward_sum 16.00, number of steps 5839, number of wins 21, number of losses 5\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 18850, reward_sum 8.00, number of steps 8345, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 18900, reward_sum 7.00, number of steps 8533, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 18950, reward_sum 10.00, number of steps 5388, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 19000, reward_sum 14.00, number of steps 6181, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 19050, reward_sum 10.00, number of steps 6806, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 19100, reward_sum 12.00, number of steps 5346, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 19150, reward_sum 9.00, number of steps 8039, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 19200, reward_sum 5.00, number of steps 7696, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 19250, reward_sum -3.00, number of steps 9016, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 19300, reward_sum 13.00, number of steps 4852, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 19350, reward_sum 7.00, number of steps 7021, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 19400, reward_sum 14.00, number of steps 3702, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 19450, reward_sum 10.00, number of steps 7463, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 19500, reward_sum 8.00, number of steps 6349, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 19550, reward_sum 6.00, number of steps 6446, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 19600, reward_sum 7.00, number of steps 7019, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 19650, reward_sum 2.00, number of steps 9276, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 19700, reward_sum 10.00, number of steps 6359, number of wins 21, number of losses 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 19750, reward_sum 8.00, number of steps 9484, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 19800, reward_sum 12.00, number of steps 6294, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 19850, reward_sum 6.00, number of steps 6684, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 19900, reward_sum 13.00, number of steps 5709, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 19950, reward_sum 9.00, number of steps 4935, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 20000, reward_sum 7.00, number of steps 7749, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 20050, reward_sum 11.00, number of steps 4821, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 20100, reward_sum 6.00, number of steps 7934, number of wins 21, number of losses 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9c5c83c9bd44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m#Train baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbl_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbl_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                       \u001b[0mbl_advantages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeed_advantages\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tfgpu/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_file = 'reward_trend124.npy'\n",
    "win_file = 'win_trend124.npy'\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "H1 = 200 # number of hidden layer neurons\n",
    "H2 = 200\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.99 # discount factor for reward\n",
    "\n",
    "epochs = 900000 # Reduce to reasonable number if don't intend to manually stop\n",
    "\n",
    "# model initialization\n",
    "D = 80 # input dimensionality of one image\n",
    "C = 2 # class number\n",
    "\n",
    "def policy_gradient():\n",
    "''' Function to create tensorflow graph for policy gradient network'''\n",
    "    with tf.variable_scope(\"policy\"):\n",
    "        state = tf.placeholder(tf.float32, [None, D*D])\n",
    "        actions = tf.placeholder(tf.int32, [None, 1])\n",
    "        rewards = tf.placeholder(tf.float32, [None, 1])\n",
    "        \n",
    "        #One Relu layer with 200 neurons, 1 sigmoid predicting probability of going \"Right\"\n",
    "        hidden_lyr1 = tf.layers.dense(activation=tf.nn.relu, inputs= state, units=H1, \\\n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        output_lyr = tf.layers.dense(activation=tf.sigmoid, inputs = hidden_lyr1, units = 1, \\\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        \n",
    "        probabilities = output_lyr\n",
    "        #Log loss giving high weights to actions that result in more reward\n",
    "        loss = tf.losses.log_loss(\n",
    "            labels=actions,\n",
    "            predictions=output_lyr,\n",
    "            weights=rewards,\n",
    "            epsilon=1e-7)\n",
    "        #ADAM optimizer to minimize logloss\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return probabilities, state, actions, rewards, optimizer\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    #NORMALIZE REWARDS BY TAKING MEAN/SD\n",
    "    discounted_r -= np.mean(discounted_r)\n",
    "    discounted_r /= np.std(discounted_r)\n",
    "    return discounted_r\n",
    "\n",
    "def choose_action(prob):\n",
    "    probs = np.concatenate([1-prob, prob])\n",
    "    action = np.random.choice(range(len(probs)), p=probs)  # select action w.r.t the actions prob\n",
    "    return action\n",
    "\n",
    "def baseline():\n",
    "    with tf.variable_scope(\"baseline\"):\n",
    "        state = tf.placeholder(tf.float32, [None, D*D])\n",
    "        actions = tf.placeholder(tf.int32, [None, 1])\n",
    "        expected_rewards = tf.placeholder(tf.float32, [None,])\n",
    "        #misnomer because it trains on actual discounted rewards so the policy network can use expected rewards.\n",
    "\n",
    "        #2 fully connected relu layers, one linear output layer.\n",
    "        hidden_lyr1 = tf.layers.dense(activation=tf.nn.leaky_relu, inputs= state, units=H1, \\\n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        #hidden = tf.nn.relu(tf.matmul(state, params_w1) + params_b1)\n",
    "        hidden_lyr2 = tf.layers.dense(activation=tf.nn.relu, inputs= hidden_lyr1, units=H2,\\\n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        output_lyr = tf.layers.dense(activation=None, inputs = hidden_lyr2, units = 1, \\\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Square error loss\n",
    "        loss = tf.reduce_mean(tf.square(output_lyr - expected_rewards), name=\"mse\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate*1.2).minimize(loss)\n",
    "        return output_lyr, state, actions, expected_rewards, optimizer\n",
    "\n",
    "\n",
    "\n",
    "policy_grad = policy_gradient()\n",
    "baseline_obj = baseline()\n",
    "\n",
    "sess = tf.InteractiveSession() \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "\n",
    "reward_sum = 0\n",
    "reward_trend = []\n",
    "wins_trend = []\n",
    "totepisodes = 0\n",
    "for epoch in range(epochs):\n",
    "    pl_calculated, pl_state, pl_actions, pl_advantages, pl_optimizer = policy_grad\n",
    "    bl_calculated, bl_state, bl_actions, bl_advantages, bl_optimizer = baseline_obj\n",
    "    feed_states, feed_actions, feed_reward, feed_advantages = [], [], [], []\n",
    "    #Batch size of 2 episodes\n",
    "    for episode_number in range(2):\n",
    "        feed_reward = []\n",
    "        observation2 = env.reset()\n",
    "       \n",
    "        reward_sum = 0\n",
    "        \n",
    "        observation = np.zeros(D*D)\n",
    "        observation2 = preprocess(observation2)\n",
    "        old_obs = None\n",
    "        #Run till end of each episode\n",
    "        while True:\n",
    "            #STATE IS CHANGE BETWEEN CONSECUTIVE FRAMES\n",
    "            full_obs = observation2 - observation\n",
    "            \n",
    "            state = np.copy(full_obs) #shape (D*D)\n",
    "            #Use policy network to get action probability\n",
    "            aprob = sess.run(pl_calculated, feed_dict={pl_state: np.reshape(state, (1, D*D))}) # aprob's shape: 1 * Cnp.reshape(state, (1, D*D)\n",
    "            action = choose_action(aprob[0]) + 2 # select an action based on policy gradient - Plus 2 to make action 2 or 3\n",
    "            feed_states.append(state)\n",
    "            #Store 0-1 actions instead of 2-3\n",
    "            feed_actions.append(action - 2)\n",
    "            #Reset observation\n",
    "            observation = np.copy(observation2)\n",
    "            # step the environment and get new measurements\n",
    "            observation2, reward, done, info = env.step(action)\n",
    "            observation2 = preprocess(observation2)\n",
    "            \n",
    "            reward_sum += reward\n",
    "            feed_reward.append(reward)\n",
    "\n",
    "            if done: # an episode finished\n",
    "                if totepisodes % 50 == 0:\n",
    "                    print(\"episode is done\")\n",
    "                    print(\"reward_sum: {}\".format(reward_sum))\n",
    "\n",
    "                    \n",
    "                    np.save(reward_file,np.array(reward_trend))\n",
    "                    np.save(win_file,np.array(wins_trend))\n",
    "                    print(\"episode %d, reward_sum %.2f, number of steps %d, number of wins %d, number of losses %d\" \\\n",
    "                          % (totepisodes,reward_sum, len(feed_reward), feed_reward.count(1),feed_reward.count(-1)))\n",
    "\n",
    "\n",
    "                reward_trend.append(reward_sum)\n",
    "                wins_trend.append(feed_reward.count(1))\n",
    "                feed_advantages.append(discount_rewards(feed_reward)) # compute discounted and normalized rewards\n",
    "                totepisodes  += 1\n",
    "\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #Get Expected reward from state through baseline network\n",
    "    bl_adj = sess.run(bl_calculated, feed_dict={bl_state: np.stack(feed_states, axis=0), \\\n",
    "                                      })\n",
    "    \n",
    "    feed_advantages = np.concatenate(feed_advantages)\n",
    "    #Subtract baseline adjustment to reduce variance without affecting bias\n",
    "    exp_adj_rewards = feed_advantages - np.hstack(bl_adj)\n",
    "    \n",
    "    \n",
    "    #Train baseline with current rewards\n",
    "    sess.run(bl_optimizer, feed_dict={bl_state: np.stack(feed_states, axis=0), \\\n",
    "                                      bl_advantages: feed_advantages})\n",
    "\n",
    "    #Train policy network with adjusted rewards\n",
    "    sess.run(pl_optimizer, feed_dict={pl_state: np.stack(feed_states, axis=0), \\\n",
    "                                      pl_advantages: np.vstack(exp_adj_rewards), pl_actions: np.vstack(feed_actions)})\n",
    "\n",
    "#Plot performance curve when done    \n",
    "plt.clf()\n",
    "plt.plot(reward_trend, color = 'blue')\n",
    "plt.plot(np.convolve(reward_trend, np.ones((10,))/10, mode='valid'),color = 'red')\n",
    "plt.title('Pong Reward vs. Episode')\n",
    "plt.savefig('Pong_reward_trend.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_store/policy_network.ckpt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(\"model_store\", \"policy_network.ckpt\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.plot(reward_trend, color = 'blue')\n",
    "plt.plot(np.convolve(reward_trend, np.ones((100,))/100, mode='valid'),color = 'red')\n",
    "plt.title('Pong Reward vs. Episode')\n",
    "plt.savefig('pong_reward_trend_stopped.png')\n",
    "#plt.plot(reward_trend)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
