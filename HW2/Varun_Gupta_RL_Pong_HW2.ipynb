{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#SELECT GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    #Change reshape to return flattened vector\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create  game environment and examine action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\") #choose the game\n",
    "\n",
    "obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 0, reward_sum -21.00, number of steps 1347, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 50, reward_sum -20.00, number of steps 1380, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 100, reward_sum -20.00, number of steps 1229, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 150, reward_sum -21.00, number of steps 1415, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 200, reward_sum -20.00, number of steps 1470, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 250, reward_sum -21.00, number of steps 1663, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 300, reward_sum -20.00, number of steps 1322, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 350, reward_sum -20.00, number of steps 2033, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -19.0\n",
      "episode 400, reward_sum -19.00, number of steps 1678, number of wins 2, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 450, reward_sum -21.00, number of steps 1747, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 500, reward_sum -20.00, number of steps 1973, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 550, reward_sum -20.00, number of steps 2033, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 600, reward_sum -20.00, number of steps 1612, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -20.0\n",
      "episode 650, reward_sum -20.00, number of steps 1969, number of wins 1, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 700, reward_sum -21.00, number of steps 1813, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 750, reward_sum -17.00, number of steps 2794, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -19.0\n",
      "episode 800, reward_sum -19.00, number of steps 2312, number of wins 2, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -19.0\n",
      "episode 850, reward_sum -19.00, number of steps 2012, number of wins 2, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 900, reward_sum -21.00, number of steps 2006, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -18.0\n",
      "episode 950, reward_sum -18.00, number of steps 2135, number of wins 3, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1000, reward_sum -15.00, number of steps 2349, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1050, reward_sum -15.00, number of steps 2430, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 1100, reward_sum -17.00, number of steps 2549, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1150, reward_sum -15.00, number of steps 3891, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -21.0\n",
      "episode 1200, reward_sum -21.00, number of steps 2790, number of wins 0, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1250, reward_sum -16.00, number of steps 2926, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 1300, reward_sum -17.00, number of steps 3215, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 1350, reward_sum -12.00, number of steps 3557, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1400, reward_sum -15.00, number of steps 2417, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 1450, reward_sum -17.00, number of steps 3209, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 1500, reward_sum -12.00, number of steps 3344, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1550, reward_sum -16.00, number of steps 3461, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -19.0\n",
      "episode 1600, reward_sum -19.00, number of steps 2824, number of wins 2, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1650, reward_sum -16.00, number of steps 3450, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -18.0\n",
      "episode 1700, reward_sum -18.00, number of steps 3776, number of wins 3, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1750, reward_sum -16.00, number of steps 3095, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 1800, reward_sum -16.00, number of steps 3194, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 1850, reward_sum -13.00, number of steps 4211, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 1900, reward_sum -13.00, number of steps 5121, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 1950, reward_sum -15.00, number of steps 4375, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 2000, reward_sum -16.00, number of steps 4562, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 2050, reward_sum -10.00, number of steps 5168, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 2100, reward_sum -15.00, number of steps 3967, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 2150, reward_sum -17.00, number of steps 3315, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 2200, reward_sum -13.00, number of steps 4264, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2250, reward_sum -12.00, number of steps 5484, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 2300, reward_sum -10.00, number of steps 5535, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 2350, reward_sum -10.00, number of steps 6335, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 2400, reward_sum -11.00, number of steps 5775, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 2450, reward_sum -15.00, number of steps 4996, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2500, reward_sum -12.00, number of steps 6075, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2550, reward_sum -12.00, number of steps 4752, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 2600, reward_sum -11.00, number of steps 5716, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 2650, reward_sum -3.00, number of steps 5729, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -15.0\n",
      "episode 2700, reward_sum -15.00, number of steps 5125, number of wins 6, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2750, reward_sum -12.00, number of steps 5368, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 2800, reward_sum 3.00, number of steps 6486, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 2850, reward_sum -11.00, number of steps 5888, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 2900, reward_sum -12.00, number of steps 5161, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 2950, reward_sum -9.00, number of steps 6945, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 3000, reward_sum -16.00, number of steps 4814, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 3050, reward_sum -17.00, number of steps 3871, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 3100, reward_sum -16.00, number of steps 4078, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -18.0\n",
      "episode 3150, reward_sum -18.00, number of steps 4390, number of wins 3, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -14.0\n",
      "episode 3200, reward_sum -14.00, number of steps 5406, number of wins 7, number of losses 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 3250, reward_sum -17.00, number of steps 3547, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 3300, reward_sum -1.00, number of steps 7440, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 3350, reward_sum -13.00, number of steps 5929, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 3400, reward_sum -11.00, number of steps 5713, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 3450, reward_sum -7.00, number of steps 6867, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 3500, reward_sum -8.00, number of steps 6075, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 3550, reward_sum -12.00, number of steps 5073, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 3600, reward_sum -5.00, number of steps 7742, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 3650, reward_sum -9.00, number of steps 5802, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 3700, reward_sum -6.00, number of steps 7870, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 3750, reward_sum -9.00, number of steps 7165, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 3800, reward_sum -10.00, number of steps 6116, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 3850, reward_sum -4.00, number of steps 7831, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 3900, reward_sum -5.00, number of steps 6513, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 3950, reward_sum -10.00, number of steps 5501, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 4000, reward_sum -11.00, number of steps 5947, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 4050, reward_sum -3.00, number of steps 6720, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 4100, reward_sum 3.00, number of steps 7966, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: -13.0\n",
      "episode 4150, reward_sum -13.00, number of steps 5282, number of wins 8, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 4200, reward_sum -9.00, number of steps 6393, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 4250, reward_sum 15.00, number of steps 4331, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 4300, reward_sum -11.00, number of steps 5841, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 4350, reward_sum -6.00, number of steps 8777, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -16.0\n",
      "episode 4400, reward_sum -16.00, number of steps 6081, number of wins 5, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 4450, reward_sum -2.00, number of steps 9568, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 4500, reward_sum -8.00, number of steps 7373, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 4550, reward_sum -11.00, number of steps 6457, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 4600, reward_sum -8.00, number of steps 5239, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 4650, reward_sum -1.00, number of steps 7190, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 4700, reward_sum -1.00, number of steps 8384, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 4750, reward_sum -1.00, number of steps 7439, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 4800, reward_sum 2.00, number of steps 6741, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 4850, reward_sum 1.00, number of steps 6408, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 4900, reward_sum -9.00, number of steps 8241, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 4950, reward_sum -7.00, number of steps 6732, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 5000, reward_sum -11.00, number of steps 6015, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -11.0\n",
      "episode 5050, reward_sum -11.00, number of steps 6236, number of wins 10, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 5100, reward_sum 10.00, number of steps 5522, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 5150, reward_sum -3.00, number of steps 9162, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 5200, reward_sum -9.00, number of steps 7496, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -14.0\n",
      "episode 5250, reward_sum -14.00, number of steps 4639, number of wins 7, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 5300, reward_sum 9.00, number of steps 6111, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 5350, reward_sum -5.00, number of steps 7523, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 5400, reward_sum -12.00, number of steps 4908, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 5450, reward_sum -7.00, number of steps 6588, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -17.0\n",
      "episode 5500, reward_sum -17.00, number of steps 6060, number of wins 4, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 5550, reward_sum 2.00, number of steps 8502, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 5600, reward_sum 8.00, number of steps 7315, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 5650, reward_sum -9.00, number of steps 5747, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 5700, reward_sum -3.00, number of steps 7645, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 5750, reward_sum -5.00, number of steps 8170, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 5800, reward_sum -7.00, number of steps 7897, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 5850, reward_sum -2.00, number of steps 7076, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 5900, reward_sum 6.00, number of steps 7080, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 5950, reward_sum -1.00, number of steps 7996, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 6000, reward_sum -2.00, number of steps 8124, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 6050, reward_sum -6.00, number of steps 7369, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -14.0\n",
      "episode 6100, reward_sum -14.00, number of steps 5977, number of wins 7, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 6150, reward_sum -5.00, number of steps 6986, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 6200, reward_sum 9.00, number of steps 5331, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 6250, reward_sum 8.00, number of steps 6737, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 6300, reward_sum 12.00, number of steps 4682, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 6350, reward_sum 7.00, number of steps 7006, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 6400, reward_sum 7.00, number of steps 7191, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 6450, reward_sum 11.00, number of steps 5578, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 6500, reward_sum 8.00, number of steps 7068, number of wins 21, number of losses 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 6550, reward_sum -9.00, number of steps 6257, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 6600, reward_sum -1.00, number of steps 8916, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 6650, reward_sum -5.00, number of steps 6785, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 6700, reward_sum -5.00, number of steps 7898, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -9.0\n",
      "episode 6750, reward_sum -9.00, number of steps 8576, number of wins 12, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 6800, reward_sum 3.00, number of steps 7639, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 6850, reward_sum 8.00, number of steps 7114, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 6900, reward_sum -4.00, number of steps 7928, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 6950, reward_sum 7.00, number of steps 7663, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 7000, reward_sum 2.00, number of steps 9029, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 7050, reward_sum 1.00, number of steps 9472, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 7100, reward_sum -5.00, number of steps 8149, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 7150, reward_sum 9.00, number of steps 7127, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 7200, reward_sum -3.00, number of steps 8198, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 7250, reward_sum 10.00, number of steps 4708, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 7300, reward_sum -5.00, number of steps 7008, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 7350, reward_sum 6.00, number of steps 9623, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 7400, reward_sum -12.00, number of steps 5897, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 7450, reward_sum -8.00, number of steps 7432, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 7500, reward_sum 4.00, number of steps 9085, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 7550, reward_sum -1.00, number of steps 9224, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 7600, reward_sum -3.00, number of steps 10000, number of wins 17, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 7650, reward_sum -5.00, number of steps 7211, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 7700, reward_sum -7.00, number of steps 6920, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 7750, reward_sum -2.00, number of steps 9827, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 7800, reward_sum -3.00, number of steps 9158, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 7850, reward_sum 11.00, number of steps 6952, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 7900, reward_sum 2.00, number of steps 9174, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 7950, reward_sum 6.00, number of steps 7866, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 8000, reward_sum 8.00, number of steps 6021, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 8050, reward_sum 5.00, number of steps 8437, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 8100, reward_sum 9.00, number of steps 6435, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 8150, reward_sum -2.00, number of steps 8920, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 8200, reward_sum -2.00, number of steps 8972, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 8250, reward_sum 8.00, number of steps 7304, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 8300, reward_sum 6.00, number of steps 6585, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 8350, reward_sum 7.00, number of steps 6297, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 8400, reward_sum 6.00, number of steps 7725, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 8450, reward_sum 4.00, number of steps 7328, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 8500, reward_sum 5.00, number of steps 7758, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 8550, reward_sum 15.00, number of steps 5869, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 8600, reward_sum 7.00, number of steps 8751, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 8650, reward_sum 3.00, number of steps 7721, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 8700, reward_sum -3.00, number of steps 6158, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 8750, reward_sum -6.00, number of steps 7954, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 8800, reward_sum -1.00, number of steps 8165, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 8850, reward_sum 4.00, number of steps 7599, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 8900, reward_sum -8.00, number of steps 8681, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 8950, reward_sum -3.00, number of steps 10000, number of wins 15, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 9000, reward_sum 3.00, number of steps 8164, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 9050, reward_sum 7.00, number of steps 7790, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 9100, reward_sum 2.00, number of steps 8759, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 9150, reward_sum -4.00, number of steps 7476, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 9200, reward_sum -7.00, number of steps 9188, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 9250, reward_sum 4.00, number of steps 7572, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 9300, reward_sum -4.00, number of steps 9021, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 9350, reward_sum 14.00, number of steps 3870, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 9400, reward_sum 5.00, number of steps 7145, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 9450, reward_sum 7.00, number of steps 7589, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 9500, reward_sum 6.00, number of steps 6996, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 9550, reward_sum 1.00, number of steps 8183, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 9600, reward_sum -2.00, number of steps 8423, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 9650, reward_sum -6.00, number of steps 8449, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 9700, reward_sum 15.00, number of steps 6024, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 9750, reward_sum 2.00, number of steps 8247, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 9800, reward_sum 4.00, number of steps 7113, number of wins 21, number of losses 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 9850, reward_sum 13.00, number of steps 6241, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 9900, reward_sum 1.00, number of steps 7749, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 16.0\n",
      "episode 9950, reward_sum 16.00, number of steps 4607, number of wins 21, number of losses 5\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 10000, reward_sum 1.00, number of steps 8058, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 10050, reward_sum 11.00, number of steps 5392, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 10100, reward_sum 6.00, number of steps 8678, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 18.0\n",
      "episode 10150, reward_sum 18.00, number of steps 4120, number of wins 21, number of losses 3\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 10200, reward_sum 7.00, number of steps 9081, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 10250, reward_sum 7.00, number of steps 8057, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 10300, reward_sum -4.00, number of steps 8398, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 10350, reward_sum 4.00, number of steps 7569, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 10400, reward_sum 9.00, number of steps 6154, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 10450, reward_sum 10.00, number of steps 6078, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 10500, reward_sum 1.00, number of steps 9089, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: -12.0\n",
      "episode 10550, reward_sum -12.00, number of steps 7659, number of wins 9, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 10600, reward_sum 6.00, number of steps 8430, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 10650, reward_sum 5.00, number of steps 7477, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 10700, reward_sum 10.00, number of steps 6419, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 10750, reward_sum 8.00, number of steps 6349, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 10800, reward_sum 13.00, number of steps 6802, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 10850, reward_sum -7.00, number of steps 9147, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 10900, reward_sum -10.00, number of steps 6642, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 10950, reward_sum -1.00, number of steps 9308, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 11000, reward_sum 8.00, number of steps 5681, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: -8.0\n",
      "episode 11050, reward_sum -8.00, number of steps 6785, number of wins 13, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 11100, reward_sum -2.00, number of steps 8165, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 11150, reward_sum -7.00, number of steps 7883, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 11200, reward_sum 2.00, number of steps 7407, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 11250, reward_sum -2.00, number of steps 7569, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 11300, reward_sum 4.00, number of steps 8029, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 11350, reward_sum -1.00, number of steps 8538, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 11400, reward_sum 6.00, number of steps 7387, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 11450, reward_sum 2.00, number of steps 6221, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 11500, reward_sum 8.00, number of steps 7380, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 11550, reward_sum 3.00, number of steps 6908, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 11600, reward_sum 15.00, number of steps 5672, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 11650, reward_sum 5.00, number of steps 6815, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 11700, reward_sum 10.00, number of steps 5562, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 11750, reward_sum 1.00, number of steps 8630, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 11800, reward_sum 4.00, number of steps 8220, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 11850, reward_sum 2.00, number of steps 7616, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 11900, reward_sum 4.00, number of steps 6667, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 11950, reward_sum 5.00, number of steps 8197, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 12000, reward_sum 12.00, number of steps 6727, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 17.0\n",
      "episode 12050, reward_sum 17.00, number of steps 4124, number of wins 21, number of losses 4\n",
      "episode is done\n",
      "reward_sum: -7.0\n",
      "episode 12100, reward_sum -7.00, number of steps 7299, number of wins 14, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 12150, reward_sum 1.00, number of steps 10000, number of wins 20, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 12200, reward_sum 4.00, number of steps 6963, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 12250, reward_sum 9.00, number of steps 7949, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 12300, reward_sum 13.00, number of steps 5689, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: -6.0\n",
      "episode 12350, reward_sum -6.00, number of steps 7908, number of wins 15, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 12400, reward_sum 6.00, number of steps 8548, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 12450, reward_sum -1.00, number of steps 8829, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 12500, reward_sum 5.00, number of steps 8500, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 12550, reward_sum 8.00, number of steps 7328, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 12600, reward_sum 9.00, number of steps 7540, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 12650, reward_sum 8.00, number of steps 6805, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 12700, reward_sum 2.00, number of steps 7573, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 12750, reward_sum 13.00, number of steps 5715, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 12800, reward_sum 12.00, number of steps 5860, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 12850, reward_sum 12.00, number of steps 6190, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 12900, reward_sum 10.00, number of steps 6137, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 12950, reward_sum 1.00, number of steps 8846, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 13000, reward_sum 10.00, number of steps 6179, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 13050, reward_sum 10.00, number of steps 5987, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 13100, reward_sum 2.00, number of steps 10000, number of wins 20, number of losses 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 13150, reward_sum 7.00, number of steps 7401, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 13200, reward_sum 8.00, number of steps 6689, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 13250, reward_sum 7.00, number of steps 7290, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 13300, reward_sum 8.00, number of steps 7279, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 13350, reward_sum 8.00, number of steps 8212, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 13400, reward_sum -2.00, number of steps 8327, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 16.0\n",
      "episode 13450, reward_sum 16.00, number of steps 7160, number of wins 21, number of losses 5\n",
      "episode is done\n",
      "reward_sum: -4.0\n",
      "episode 13500, reward_sum -4.00, number of steps 7741, number of wins 17, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 13550, reward_sum 3.00, number of steps 7494, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 13600, reward_sum 4.00, number of steps 8736, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 13650, reward_sum 12.00, number of steps 5582, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 13700, reward_sum 6.00, number of steps 6761, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 13750, reward_sum -3.00, number of steps 8918, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 13800, reward_sum 7.00, number of steps 7252, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 13850, reward_sum 5.00, number of steps 8190, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 13900, reward_sum 6.00, number of steps 7169, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 13950, reward_sum 5.00, number of steps 6885, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 14000, reward_sum 10.00, number of steps 6723, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 14050, reward_sum 2.00, number of steps 9433, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 14100, reward_sum 5.00, number of steps 9177, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 14150, reward_sum 2.00, number of steps 9938, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 14200, reward_sum 11.00, number of steps 6468, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: -10.0\n",
      "episode 14250, reward_sum -10.00, number of steps 7122, number of wins 11, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 14300, reward_sum 9.00, number of steps 6221, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 14350, reward_sum 7.00, number of steps 7198, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 14400, reward_sum -1.00, number of steps 9167, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 14450, reward_sum 4.00, number of steps 7347, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 14500, reward_sum 8.00, number of steps 8575, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 14550, reward_sum 11.00, number of steps 5310, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 14600, reward_sum 11.00, number of steps 8701, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 14650, reward_sum 6.00, number of steps 10000, number of wins 17, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 14700, reward_sum 1.00, number of steps 7590, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 14750, reward_sum 3.00, number of steps 8901, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 14800, reward_sum 6.00, number of steps 7656, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 14850, reward_sum 11.00, number of steps 6273, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 14900, reward_sum 13.00, number of steps 6086, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 14950, reward_sum 5.00, number of steps 8768, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 15000, reward_sum 5.00, number of steps 6670, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 15050, reward_sum 4.00, number of steps 8211, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 15100, reward_sum -5.00, number of steps 9371, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 15150, reward_sum 2.00, number of steps 8332, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 15200, reward_sum 4.00, number of steps 8186, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 15250, reward_sum 6.00, number of steps 7546, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 15300, reward_sum 8.00, number of steps 6979, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 15350, reward_sum 3.00, number of steps 8281, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 15400, reward_sum 8.00, number of steps 6157, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 15450, reward_sum 11.00, number of steps 5556, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 15500, reward_sum 12.00, number of steps 6590, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: -2.0\n",
      "episode 15550, reward_sum -2.00, number of steps 9545, number of wins 19, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 15600, reward_sum 11.00, number of steps 7259, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 15650, reward_sum 5.00, number of steps 7648, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 15700, reward_sum 7.00, number of steps 7288, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 15750, reward_sum 12.00, number of steps 8683, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 15800, reward_sum 15.00, number of steps 4953, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 15850, reward_sum 11.00, number of steps 6540, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 15900, reward_sum 8.00, number of steps 7075, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 15950, reward_sum 6.00, number of steps 8148, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 16.0\n",
      "episode 16000, reward_sum 16.00, number of steps 4756, number of wins 21, number of losses 5\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 16050, reward_sum 1.00, number of steps 8565, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 16100, reward_sum 15.00, number of steps 5292, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 16150, reward_sum 9.00, number of steps 7480, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 16200, reward_sum 12.00, number of steps 5668, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 16250, reward_sum -1.00, number of steps 8074, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 16300, reward_sum 8.00, number of steps 7881, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 16350, reward_sum 3.00, number of steps 8926, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 16400, reward_sum 7.00, number of steps 7436, number of wins 21, number of losses 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 16450, reward_sum 2.00, number of steps 9087, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 17.0\n",
      "episode 16500, reward_sum 17.00, number of steps 3900, number of wins 21, number of losses 4\n",
      "episode is done\n",
      "reward_sum: -5.0\n",
      "episode 16550, reward_sum -5.00, number of steps 8432, number of wins 16, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 16600, reward_sum 5.00, number of steps 7474, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 16650, reward_sum 5.00, number of steps 7046, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 16700, reward_sum 10.00, number of steps 5568, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 16750, reward_sum 2.00, number of steps 8469, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 16800, reward_sum 6.00, number of steps 8164, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 16850, reward_sum 10.00, number of steps 5094, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 16900, reward_sum 11.00, number of steps 6522, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 16950, reward_sum 13.00, number of steps 5024, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 3.0\n",
      "episode 17000, reward_sum 3.00, number of steps 7228, number of wins 21, number of losses 18\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 17050, reward_sum 4.00, number of steps 7164, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 17100, reward_sum 8.00, number of steps 6410, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 17150, reward_sum 4.00, number of steps 8245, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 17200, reward_sum 7.00, number of steps 7334, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 17250, reward_sum 2.00, number of steps 8236, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 17300, reward_sum -1.00, number of steps 8486, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 17350, reward_sum 11.00, number of steps 6318, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 17400, reward_sum 8.00, number of steps 7192, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 17450, reward_sum 9.00, number of steps 6704, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 17500, reward_sum 6.00, number of steps 8908, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 17550, reward_sum 10.00, number of steps 5970, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 17600, reward_sum 12.00, number of steps 6222, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 17650, reward_sum 7.00, number of steps 6304, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 17700, reward_sum 10.00, number of steps 5493, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 17750, reward_sum 10.00, number of steps 6312, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 1.0\n",
      "episode 17800, reward_sum 1.00, number of steps 7597, number of wins 21, number of losses 20\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 17850, reward_sum 10.00, number of steps 6134, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 17900, reward_sum 9.00, number of steps 7313, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 11.0\n",
      "episode 17950, reward_sum 11.00, number of steps 5462, number of wins 21, number of losses 10\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 18000, reward_sum -1.00, number of steps 9071, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 18050, reward_sum 12.00, number of steps 5369, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 18100, reward_sum 8.00, number of steps 8079, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 18150, reward_sum 9.00, number of steps 5562, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 18200, reward_sum 10.00, number of steps 7051, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 18250, reward_sum 13.00, number of steps 6027, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 18300, reward_sum 6.00, number of steps 7474, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 18350, reward_sum 14.00, number of steps 5496, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 18400, reward_sum 12.00, number of steps 6405, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 15.0\n",
      "episode 18450, reward_sum 15.00, number of steps 5803, number of wins 21, number of losses 6\n",
      "episode is done\n",
      "reward_sum: -1.0\n",
      "episode 18500, reward_sum -1.00, number of steps 7173, number of wins 20, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 18550, reward_sum 14.00, number of steps 4486, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 18600, reward_sum 4.00, number of steps 7553, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 18650, reward_sum 2.00, number of steps 9342, number of wins 21, number of losses 19\n",
      "episode is done\n",
      "reward_sum: 4.0\n",
      "episode 18700, reward_sum 4.00, number of steps 7066, number of wins 21, number of losses 17\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 18750, reward_sum 9.00, number of steps 7007, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 16.0\n",
      "episode 18800, reward_sum 16.00, number of steps 5839, number of wins 21, number of losses 5\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 18850, reward_sum 8.00, number of steps 8345, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 18900, reward_sum 7.00, number of steps 8533, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 18950, reward_sum 10.00, number of steps 5388, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 19000, reward_sum 14.00, number of steps 6181, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 19050, reward_sum 10.00, number of steps 6806, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 12.0\n",
      "episode 19100, reward_sum 12.00, number of steps 5346, number of wins 21, number of losses 9\n",
      "episode is done\n",
      "reward_sum: 9.0\n",
      "episode 19150, reward_sum 9.00, number of steps 8039, number of wins 21, number of losses 12\n",
      "episode is done\n",
      "reward_sum: 5.0\n",
      "episode 19200, reward_sum 5.00, number of steps 7696, number of wins 21, number of losses 16\n",
      "episode is done\n",
      "reward_sum: -3.0\n",
      "episode 19250, reward_sum -3.00, number of steps 9016, number of wins 18, number of losses 21\n",
      "episode is done\n",
      "reward_sum: 13.0\n",
      "episode 19300, reward_sum 13.00, number of steps 4852, number of wins 21, number of losses 8\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 19350, reward_sum 7.00, number of steps 7021, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 14.0\n",
      "episode 19400, reward_sum 14.00, number of steps 3702, number of wins 21, number of losses 7\n",
      "episode is done\n",
      "reward_sum: 10.0\n",
      "episode 19450, reward_sum 10.00, number of steps 7463, number of wins 21, number of losses 11\n",
      "episode is done\n",
      "reward_sum: 8.0\n",
      "episode 19500, reward_sum 8.00, number of steps 6349, number of wins 21, number of losses 13\n",
      "episode is done\n",
      "reward_sum: 6.0\n",
      "episode 19550, reward_sum 6.00, number of steps 6446, number of wins 21, number of losses 15\n",
      "episode is done\n",
      "reward_sum: 7.0\n",
      "episode 19600, reward_sum 7.00, number of steps 7019, number of wins 21, number of losses 14\n",
      "episode is done\n",
      "reward_sum: 2.0\n",
      "episode 19650, reward_sum 2.00, number of steps 9276, number of wins 21, number of losses 19\n"
     ]
    }
   ],
   "source": [
    "reward_file = 'reward_trend124.npy'\n",
    "win_file = 'win_trend124.npy'\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "H1 = 200 # number of hidden layer neurons\n",
    "H2 = 200\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.99 # discount factor for reward\n",
    "\n",
    "epochs = 900000 # Reduce to reasonable number if don't intend to manually stop\n",
    "\n",
    "# model initialization\n",
    "D = 80 # input dimensionality of one image\n",
    "C = 2 # class number\n",
    "\n",
    "def policy_gradient():\n",
    "''' Function to create tensorflow graph for policy gradient network'''\n",
    "    with tf.variable_scope(\"policy\"):\n",
    "        state = tf.placeholder(tf.float32, [None, D*D])\n",
    "        actions = tf.placeholder(tf.int32, [None, 1])\n",
    "        rewards = tf.placeholder(tf.float32, [None, 1])\n",
    "        \n",
    "        #One Relu layer with 200 neurons, 1 sigmoid predicting probability of going \"Right\"\n",
    "        hidden_lyr1 = tf.layers.dense(activation=tf.nn.relu, inputs= state, units=H1, \\\n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        output_lyr = tf.layers.dense(activation=tf.sigmoid, inputs = hidden_lyr1, units = 1, \\\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        \n",
    "        probabilities = output_lyr\n",
    "        #Log loss giving high weights to actions that result in more reward\n",
    "        loss = tf.losses.log_loss(\n",
    "            labels=actions,\n",
    "            predictions=output_lyr,\n",
    "            weights=rewards,\n",
    "            epsilon=1e-7)\n",
    "        #ADAM optimizer to minimize logloss\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return probabilities, state, actions, rewards, optimizer\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    #NORMALIZE REWARDS BY TAKING MEAN/SD\n",
    "    discounted_r -= np.mean(discounted_r)\n",
    "    discounted_r /= np.std(discounted_r)\n",
    "    return discounted_r\n",
    "\n",
    "def choose_action(prob):\n",
    "    probs = np.concatenate([1-prob, prob])\n",
    "    action = np.random.choice(range(len(probs)), p=probs)  # select action w.r.t the actions prob\n",
    "    return action\n",
    "\n",
    "def baseline():\n",
    "    with tf.variable_scope(\"baseline\"):\n",
    "        state = tf.placeholder(tf.float32, [None, D*D])\n",
    "        actions = tf.placeholder(tf.int32, [None, 1])\n",
    "        expected_rewards = tf.placeholder(tf.float32, [None,])\n",
    "        #misnomer because it trains on actual discounted rewards so the policy network can use expected rewards.\n",
    "\n",
    "        #2 fully connected relu layers, one linear output layer.\n",
    "        hidden_lyr1 = tf.layers.dense(activation=tf.nn.leaky_relu, inputs= state, units=H1, \\\n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        #hidden = tf.nn.relu(tf.matmul(state, params_w1) + params_b1)\n",
    "        hidden_lyr2 = tf.layers.dense(activation=tf.nn.relu, inputs= hidden_lyr1, units=H2,\\\n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        output_lyr = tf.layers.dense(activation=None, inputs = hidden_lyr2, units = 1, \\\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Square error loss\n",
    "        loss = tf.reduce_mean(tf.square(output_lyr - expected_rewards), name=\"mse\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate*1.2).minimize(loss)\n",
    "        return output_lyr, state, actions, expected_rewards, optimizer\n",
    "\n",
    "\n",
    "\n",
    "policy_grad = policy_gradient()\n",
    "baseline_obj = baseline()\n",
    "\n",
    "sess = tf.InteractiveSession() \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "\n",
    "reward_sum = 0\n",
    "reward_trend = []\n",
    "wins_trend = []\n",
    "totepisodes = 0\n",
    "for epoch in range(epochs):\n",
    "    pl_calculated, pl_state, pl_actions, pl_advantages, pl_optimizer = policy_grad\n",
    "    bl_calculated, bl_state, bl_actions, bl_advantages, bl_optimizer = baseline_obj\n",
    "    feed_states, feed_actions, feed_reward, feed_advantages = [], [], [], []\n",
    "    #Batch size of 2 episodes\n",
    "    for episode_number in range(2):\n",
    "        feed_reward = []\n",
    "        observation2 = env.reset()\n",
    "       \n",
    "        reward_sum = 0\n",
    "        \n",
    "        observation = np.zeros(D*D)\n",
    "        observation2 = preprocess(observation2)\n",
    "        old_obs = None\n",
    "        #Run till end of each episode\n",
    "        while True:\n",
    "            #STATE IS CHANGE BETWEEN CONSECUTIVE FRAMES\n",
    "            full_obs = observation2 - observation\n",
    "            \n",
    "            state = np.copy(full_obs) #shape (D*D)\n",
    "            #Use policy network to get action probability\n",
    "            aprob = sess.run(pl_calculated, feed_dict={pl_state: np.reshape(state, (1, D*D))}) # aprob's shape: 1 * Cnp.reshape(state, (1, D*D)\n",
    "            action = choose_action(aprob[0]) + 2 # select an action based on policy gradient - Plus 2 to make action 2 or 3\n",
    "            feed_states.append(state)\n",
    "            #Store 0-1 actions instead of 2-3\n",
    "            feed_actions.append(action - 2)\n",
    "            #Reset observation\n",
    "            observation = np.copy(observation2)\n",
    "            # step the environment and get new measurements\n",
    "            observation2, reward, done, info = env.step(action)\n",
    "            observation2 = preprocess(observation2)\n",
    "            \n",
    "            reward_sum += reward\n",
    "            feed_reward.append(reward)\n",
    "\n",
    "            if done: # an episode finished\n",
    "                if totepisodes % 50 == 0:\n",
    "                    print(\"episode is done\")\n",
    "                    print(\"reward_sum: {}\".format(reward_sum))\n",
    "\n",
    "                    \n",
    "                    np.save(reward_file,np.array(reward_trend))\n",
    "                    np.save(win_file,np.array(wins_trend))\n",
    "                    print(\"episode %d, reward_sum %.2f, number of steps %d, number of wins %d, number of losses %d\" \\\n",
    "                          % (totepisodes,reward_sum, len(feed_reward), feed_reward.count(1),feed_reward.count(-1)))\n",
    "\n",
    "\n",
    "                reward_trend.append(reward_sum)\n",
    "                wins_trend.append(feed_reward.count(1))\n",
    "                feed_advantages.append(discount_rewards(feed_reward)) # compute discounted and normalized rewards\n",
    "                totepisodes  += 1\n",
    "\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #Get Expected reward from state through baseline network\n",
    "    bl_adj = sess.run(bl_calculated, feed_dict={bl_state: np.stack(feed_states, axis=0), \\\n",
    "                                      })\n",
    "    \n",
    "    feed_advantages = np.concatenate(feed_advantages)\n",
    "    #Subtract baseline adjustment to reduce variance without affecting bias\n",
    "    exp_adj_rewards = feed_advantages - np.hstack(bl_adj)\n",
    "    \n",
    "    \n",
    "    #Train baseline with current rewards\n",
    "    sess.run(bl_optimizer, feed_dict={bl_state: np.stack(feed_states, axis=0), \\\n",
    "                                      bl_advantages: feed_advantages})\n",
    "\n",
    "    #Train policy network with adjusted rewards\n",
    "    sess.run(pl_optimizer, feed_dict={pl_state: np.stack(feed_states, axis=0), \\\n",
    "                                      pl_advantages: np.vstack(exp_adj_rewards), pl_actions: np.vstack(feed_actions)})\n",
    "\n",
    "#Plot performance curve when done    \n",
    "plt.clf()\n",
    "plt.plot(reward_trend, color = 'blue')\n",
    "plt.plot(np.convolve(reward_trend, np.ones((10,))/10, mode='valid'),color = 'red')\n",
    "plt.title('Pong Reward vs. Episode')\n",
    "plt.savefig('Pong_reward_trend.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(reward_trend, color = 'blue')\n",
    "plt.plot(np.convolve(reward_trend, np.ones((10,))/10, mode='valid'),color = 'red')\n",
    "plt.title('Cartpole Reward vs. Episode')\n",
    "plt.savefig('cartpole_reward_trend.png')\n",
    "#plt.plot(reward_trend)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
