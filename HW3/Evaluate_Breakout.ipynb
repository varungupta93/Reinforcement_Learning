{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "#SELECT GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess function from tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    ''' prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array '''\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else just set to 1\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80])\n",
    "\n",
    "env = gym.envs.make(\"Breakout-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(q_predictions, q_X, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "    Args:\n",
    "        q_predictions: The tf node that gets q predictions for each action given a state\n",
    "        q_X: The tf node that is the input placeholder for the network (state)\n",
    "        nA: Number of actions in the environment.\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        #q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        #Get q values for the various actions\n",
    "        q_values = sess.run(q_predictions, feed_dict={q_X : np.expand_dims(observation,0)})[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        #Use the linearly decreasing explore factor for the suboptimal actions, and 1-epsilon for optimal\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 1000\n",
    "experiment_dir = os.path.relpath(\"./breakout_experiments\")\n",
    "checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "poss_actions = list(range(4))\n",
    "learning_rate = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(scope):\n",
    "        \"\"\"\n",
    "        Function to build a tensorflow graph and return its components\n",
    "        \"\"\"\n",
    "        #This folder will store the summaries folder.\n",
    "        summaries_dir = \"./breakout_summaries\"\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            # Placeholders for our input\n",
    "        \n",
    "            X_pl = tf.placeholder(shape=[None, 80, 80,4], dtype=tf.float32, name=\"X\")\n",
    "            # The TD target value\n",
    "            y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "            # Index of selected action\n",
    "            actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "            \n",
    "            X = X_pl\n",
    "            batch_size = tf.shape(X_pl)[0]\n",
    "            \n",
    "            # Three convolutional layers\n",
    "            conv1 = tf.contrib.layers.conv2d(\n",
    "                X, 32, 8, 2, activation_fn=tf.nn.relu)\n",
    "            conv2 = tf.contrib.layers.conv2d(\n",
    "                conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "            conv3 = tf.contrib.layers.conv2d(\n",
    "                conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "            # Fully connected layers - 1 hiden layer and an output layer.\n",
    "            flattened = tf.contrib.layers.flatten(conv3)\n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "            predictions = tf.contrib.layers.fully_connected(fc1, len(poss_actions), activation_fn=None)\n",
    "            # Get the predictions for the chosen actions only\n",
    "#             gather_indices = tf.range(batch_size) * tf.shape(predictions)[1] + actions_pl\n",
    "#             action_predictions = tf.gather(tf.reshape(predictions, [-1]), gather_indices)\n",
    "            \n",
    "            #Only take Q for the action we take (multiply with one hot vector)\n",
    "            action_one_hot = tf.one_hot(actions_pl, len(poss_actions), 1.0, 0.0, name='action_one_hot')\n",
    "            action_predictions = tf.reduce_sum(tf.multiply(predictions, action_one_hot), axis=1)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            losses = tf.squared_difference(y_pl, action_predictions)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "            # Adam optimizer to reduce MSE\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)#, 0.99, 0.0, 1e-6)\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.contrib.framework.get_global_step())\n",
    "            \n",
    "            #Summaries for tensorboard\n",
    "            summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", loss),\n",
    "            tf.summary.histogram(\"loss_hist\", losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(predictions))\n",
    "            ])\n",
    "            \n",
    "            #Create summary_dir when building the model\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "        return predictions, train_op, X_pl, y_pl, actions_pl, summaries, summary_writer, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(q_predictions, q_X, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "    Args:\n",
    "        q_predictions: The tf node that gets q predictions for each action given a state\n",
    "        q_X: The tf node that is the input placeholder for the network (state)\n",
    "        nA: Number of actions in the environment.\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        #q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        #Get q values for the various actions\n",
    "        q_values = sess.run(q_predictions, feed_dict={q_X : np.expand_dims(observation,0)})[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        #Use the linearly decreasing explore factor for the suboptimal actions, and 1-epsilon for optimal\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        #A = q_values\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.relpath(\"./breakout_experiments\")\n",
    "# Create a global step variable for training iteration count\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "\n",
    "q_estimator = build_model(scope=\"q\")\n",
    "target_estimator = build_model(scope=\"target_q\")\n",
    "\n",
    "\n",
    "allrewards = []\n",
    "totrewards = []\n",
    "with tf.Session() as sess:\n",
    "        print(\"SESSION STARTED\")\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        q_predictions, q_train_op, q_X, q_y, q_actions, q_summaries, q_summary_writer, q_loss = q_estimator\n",
    "        target_predictions, target_train_op, target_X, target_y, target_actions,\\\n",
    "        target_summaries, t_summary_writer, target_loss = target_estimator\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        \n",
    "        new_saver = tf.train.Saver()\n",
    "        new_saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "       \n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "\n",
    "        policy = make_epsilon_greedy_policy(\n",
    "        q_predictions, q_X,\n",
    "        len(poss_actions))\n",
    "    \n",
    "        for game in range(num_games):\n",
    "            # start game \n",
    "            q_predictions, q_train_op, q_X, q_y, q_actions, q_summaries, q_summary_writer, q_loss = q_estimator\n",
    "            target_predictions, target_train_op, target_X, target_y, target_actions,\\\n",
    "            target_summaries, t_summary_writer, target_loss = target_estimator\n",
    "            state = env.reset()\n",
    "            state, _, done, _ = env.step(1)\n",
    "            state = preprocess(state)\n",
    "            #Stack the initial frame 4 times initially.\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "            ctr = 0\n",
    "            reward_total = 0\n",
    "            while True: \n",
    "                \n",
    "                q_predictions, q_train_op, q_X, q_y, q_actions, q_summaries, q_summary_writer, q_loss = q_estimator\n",
    "                #print(np.sum(state))\n",
    "                action_probs = policy(sess, state, 0.1)\n",
    "               # print(action_probs)\n",
    "#                 if ctr % 150 == 0:\n",
    "# #                     plt.gcf()\n",
    "#                     plt.figure()\n",
    "#                     plt.imshow(state[:,:,3])\n",
    "#                     plt.show()\n",
    "#                     plt.close()\n",
    "#                     print(ctr)\n",
    "                    \n",
    "                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "                #print(action)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                allrewards.append(reward)\n",
    "                next_state = preprocess(new_state)\n",
    "                #Stack new frame and pop out fourth frame.\n",
    "                next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "                \n",
    "                state = next_state.copy()\n",
    "                              \n",
    "                ctr = ctr + 1\n",
    "                if done:\n",
    "                    totrewards.append(reward_total + reward)\n",
    "                    print(\"episode \" + str(game) + \": Reward = \" + str(reward_total + reward))\n",
    "                    with open('breakout_1000_2.txt', 'a') as f:\n",
    "  \n",
    "                        f.write(str(reward_total + reward) + \"\\n\")\n",
    "                    break\n",
    "                else: \n",
    "                    # Update reward total\n",
    "                    reward_total = reward_total + reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(allrewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('breakout_1000_2.txt', 'w') as f:\n",
    "#     for rew in allrewards:\n",
    "#         f.write(str(rew) + \"\\n\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
