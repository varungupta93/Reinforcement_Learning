{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN for Breakout-V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "#SELECT GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess function from tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    ''' prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array '''\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else just set to 1\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8b2076b978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8b20701ba8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADBNJREFUeJzt3V+MXOV9xvHvUxuLJikydgAZ7NQgUQIXxaQWBVFVLa4LSRH0IlSgtEIREjdpBWqq1OSuUiuRm4RcVJEQkHJBA9QBBaEIx3KI2kqVA8RuEjCOCaV4ZQc7/BFpkNLa+fVijpsVXcdnd2dmffb9fqTRzPvOmT3v0dEz58+cPb9UFZLa8itLPQBJ02fwpQYZfKlBBl9qkMGXGmTwpQYZfKlBiwp+kuuT7E/ycpJt4xqUpMnKQi/gSbIC+AGwFZgBngVuraoXxzc8SZOwchGfvRJ4uapeAUjyCHATcNLgf3DNitq44Yxef/wH333fIoYmLS+/8Zvv9pru1YP/w4/fPJ5TTbeY4F8AHJzVngF++5d9YOOGM/j2jg29/vh1529a+MikZWbHjr29prvyuoOnnojFHePP9a3y/44bktyR5Lkkzx194/giZidpXBYT/Blg9uZ7PXDovRNV1X1VtbmqNp+zdsUiZidpXBYT/GeBi5NcmGQVcAvw5HiGJWmSFnyMX1XHkvw5sANYATxYVS+MbWSSJmYxJ/eoqq8DXx/TWCRNiVfuSQ0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDThn8JA8mOZLk+7P61iTZmeRA93z2ZIcpaZz6bPH/Abj+PX3bgF1VdTGwq2tLGohTBr+q/hl48z3dNwEPda8fAv54zOOSNEELPcY/r6oOA3TP545vSJImbeIn9yyhJZ1+Fnpf/deTrKuqw0nWAUdONmFV3QfcB7D58jN71+TecahfkUBJ87fQLf6TwG3d69uAr41nOJKmoc/PeV8B/g24JMlMktuBe4CtSQ4AW7u2pIE45a5+Vd16kre2jHkskqbEK/ekBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUF97rm3IckzSfYleSHJnV2/ZbSkgeqzxT8GfLqqLgWuAj6V5DIsoyUNVp8SWoer6jvd658A+4ALsIyWNFjzOsZPshG4AtiNZbSkweod/CQfAL4K3FVV78zjc5bQkk4zvYKf5AxGoX+4qh7vul/vymfxy8poVdV9VbW5qjafs3bFOMYsaZH6nNUP8ACwr6o+P+sty2hJA9WnaOY1wJ8B30tyopLlZxmVzXqsK6n1GnDzZIYoadz6lND6VyAnedsyWtIAeeWe1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzWozz33lsR1529a6iFIp40dh/aeeqJ56HOX3TOTfDvJv3e18/6m678wye6udt6jSVaNdWSSJqbPrv7PgGur6nJgE3B9kquAzwFf6GrnvQXcPrlhShqnPrXzqqr+q2ue0T0KuBbY3vVbO08akL6VdFZ099Q/AuwEfgi8XVXHuklmGBXSnOuzltCSTjO9gl9Vx6tqE7AeuBK4dK7JTvJZS2hJp5l5/ZxXVW8D3wKuAlYnOfGrwHrg0HiHJmlS+pzVPyfJ6u71rwJ/AOwDngE+3k1m7TxpQPr8jr8OeCjJCkZfFI9V1VNJXgQeSfK3wB5GhTUlDUCf2nnfBa6Yo/8VRsf7kgbGS3alBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUG9g9/dW39Pkqe6tiW0pIGazxb/TkZ31z3BElrSQPWtpLMe+CPg/q4dLKElDVbfLf69wGeAn3fttVhCSxqsPgU1bgCOVNXzs7vnmNQSWtJA9CmocQ1wY5KPAWcCZzHaA1idZGW31beEljQgfcpk311V66tqI3AL8M2q+gSW0JIGazG/4/818JdJXmZ0zG8JLWkg+uzq/5+q+hajarmW0JIGzCv3pAYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBvW69leRV4CfAceBYVW1OsgZ4FNgIvAr8SVW9NZlhShqn+Wzxf7+qNlXV5q69DdjVldDa1bUlDcBidvVvYlQ6CyyhJQ1K3+AX8I0kzye5o+s7r6oOA3TP505igJLGr+/tta+pqkNJzgV2Jnmp7wy6L4o7AD50wbzu5i1pQnpt8avqUPd8BHiC0f30X0+yDqB7PnKSz1o7TzrN9Cma+f4kv3biNfCHwPeBJxmVzgJLaEmD0mff+zzgiSQnpv/Hqno6ybPAY0luB14Dbp7cMCWN0ymD35XKunyO/jeALZMYlKTJ8so9qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2pQr+AnWZ1ke5KXkuxLcnWSNUl2JjnQPZ896cFKGo++W/wvAk9X1YcZ3X9vH5bQkgarz+21zwJ+F3gAoKr+u6rexhJa0mD12eJfBBwFvpxkT5L7u/vrW0JLGqg+wV8JfAT4UlVdAfyUeezWJ7kjyXNJnjv6xvEFDlPSOPUJ/gwwU1W7u/Z2Rl8EltCSBuqUwa+qHwEHk1zSdW0BXsQSWtJg9S1f+xfAw0lWAa8An2T0pWEJLWmAegW/qvYCm+d4yxJa0gB55Z7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNahPQY1Lkuyd9XgnyV2W0JKGq89ddvdX1aaq2gT8FvAu8ASW0JIGa767+luAH1bVf2IJLWmw5hv8W4CvdK8toSUNVO/gd/fUvxH4p/nMwBJa0ulnPlv8jwLfqarXu7YltKSBmk/wb+UXu/lgCS1psHoFP8n7gK3A47O67wG2JjnQvXfP+IcnaRL6ltB6F1j7nr43sISWNEheuSc1yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81qNd/50nTsOPQ3on83evO3zSRvztkbvGlBhl8qUEGX2qQwZca5Mk9nTY8CTc9bvGlBhl8qUGpqunNLDkK/BT48dRmOl0fZHkum8s1HL9eVeecaqKpBh8gyXNVtXmqM52S5bpsLtfy466+1CCDLzVoKYJ/3xLMc1qW67K5XMvM1I/xJS09d/WlBk01+EmuT7I/yctJtk1z3uOUZEOSZ5LsS/JCkju7/jVJdiY50D2fvdRjXYgkK5LsSfJU174wye5uuR5Nsmqpx7gQSVYn2Z7kpW7dXb1c1tl8TS34SVYAfw98FLgMuDXJZdOa/5gdAz5dVZcCVwGf6pZlG7Crqi4GdnXtIboT2Der/TngC91yvQXcviSjWrwvAk9X1YeByxkt43JZZ/NTVVN5AFcDO2a17wbuntb8J7xsXwO2AvuBdV3fOmD/Uo9tAcuynlEArgWeAsLoIpeVc63HoTyAs4D/oDuvNat/8OtsIY9p7upfAByc1Z7p+gYtyUbgCmA3cF5VHQbons9dupEt2L3AZ4Cfd+21wNtVdaxrD3W9XQQcBb7cHcbcn+T9LI91Nm/TDH7m6Bv0TwpJPgB8Fbirqt5Z6vEsVpIbgCNV9fzs7jkmHeJ6Wwl8BPhSVV3B6NLxNnbr5zDN4M8AG2a11wOHpjj/sUpyBqPQP1xVj3fdrydZ172/DjiyVONboGuAG5O8CjzCaHf/XmB1khP/wj3U9TYDzFTV7q69ndEXwdDX2YJMM/jPAhd3Z4hXAbcAT05x/mOTJMADwL6q+vyst54Ebute38bo2H8wquruqlpfVRsZrZ9vVtUngGeAj3eTDW65AKrqR8DBJJd0XVuAFxn4Oluoaf933scYbUFWAA9W1d9NbeZjlOR3gH8BvscvjoU/y+g4/zHgQ8BrwM1V9eaSDHKRkvwe8FdVdUOSixjtAawB9gB/WlU/W8rxLUSSTcD9wCrgFeCTjDZ+y2KdzYdX7kkN8so9qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBv0vn1IPqZErx6QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Sample preprocessed image\n",
    "test_obs = env.reset()\n",
    "processed = preprocess(test_obs)\n",
    "plt.figure()\n",
    "plt.imshow(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.contains(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE ACTION SPACE\n",
    "poss_actions = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEARNING RATE\n",
    "learning_rate = 2e-4\n",
    "q_scope = 'q'\n",
    "target_scope = 'target_q'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for the model, policy, and target update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(scope):\n",
    "        \"\"\"\n",
    "        Function to build a tensorflow graph and return its components\n",
    "        \"\"\"\n",
    "        #This folder will store the summaries folder.\n",
    "        summaries_dir = \"./breakout_summaries\"\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            # Placeholders for our input\n",
    "        \n",
    "            X_pl = tf.placeholder(shape=[None, 80, 80,4], dtype=tf.float32, name=\"X\")\n",
    "            # The TD target value\n",
    "            y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "            # Index of selected action\n",
    "            actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "            \n",
    "            X = X_pl\n",
    "            batch_size = tf.shape(X_pl)[0]\n",
    "            \n",
    "            # Three convolutional layers\n",
    "            conv1 = tf.contrib.layers.conv2d(\n",
    "                X, 32, 8, 2, activation_fn=tf.nn.relu)\n",
    "            conv2 = tf.contrib.layers.conv2d(\n",
    "                conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "            conv3 = tf.contrib.layers.conv2d(\n",
    "                conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "            # Fully connected layers - 1 hiden layer and an output layer.\n",
    "            flattened = tf.contrib.layers.flatten(conv3)\n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "            predictions = tf.contrib.layers.fully_connected(fc1, len(poss_actions), activation_fn=None)\n",
    "\n",
    "            #Only take Q for the action we take (multiply with one hot vector)\n",
    "            action_one_hot = tf.one_hot(actions_pl, len(poss_actions), 1.0, 0.0, name='action_one_hot')\n",
    "            action_predictions = tf.reduce_sum(tf.multiply(predictions, action_one_hot), axis=1)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            losses = tf.squared_difference(y_pl, action_predictions)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "            # Adam optimizer to reduce MSE\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)#, 0.99, 0.0, 1e-6)\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.contrib.framework.get_global_step())\n",
    "            \n",
    "            #Summaries for tensorboard\n",
    "            summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", loss),\n",
    "            tf.summary.histogram(\"loss_hist\", losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(predictions))\n",
    "            ])\n",
    "            \n",
    "            #Create summary_dir when building the model\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "        return predictions, train_op, X_pl, y_pl, actions_pl, summaries, summary_writer, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1_scope, estimator2_scope):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1_scope: Variables with this scope will be copied\n",
    "      estimator2_scope: Variables with this scope will be updated\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1_scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2_scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(q_predictions, q_X, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "    Args:\n",
    "        q_predictions: The tf node that gets q predictions for each action given a state\n",
    "        q_X: The tf node that is the input placeholder for the network (state)\n",
    "        nA: Number of actions in the environment.\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        #q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        #Get q values for the various actions\n",
    "        q_values = sess.run(q_predictions, feed_dict={q_X : np.expand_dims(observation,0)})[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        #Use the linearly decreasing explore factor for the suboptimal actions, and 1-epsilon for optimal\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Function where network is trained and epsilon greedy policy updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=800000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=8000,\n",
    "                    discount_factor=0.95,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.05,\n",
    "                    epsilon_decay_steps=750000,\n",
    "                    batch_size=32):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: tuple containing relevant nodes for the q-network\n",
    "        target_estimator: tuple containing relevant nodes for the target network\n",
    "        num_episodes: Number of episodes to train for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory (max number of steps)\n",
    "        replay_memory_init_size: Number of random experiences to sample when initializing \n",
    "          the replay memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every _ steps.\n",
    "        discount_factor: Lambda time discount factor (Provided for HW)\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    #Store the graph nodes as variables\n",
    "    q_predictions, q_train_op, q_X, q_y, q_actions, q_summaries, q_summary_writer, q_loss = q_estimator\n",
    "    target_predictions, target_train_op, target_X, target_y, target_actions,\\\n",
    "    target_summaries, t_summary_writer, target_loss = target_estimator\n",
    "    #Transitions that will be stored in replay memory\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    tot_episode_rewards = []\n",
    "    tot_episode_lengths = []\n",
    "    \n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    " \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    # The epsilon decay schedule - LINEARLY DECREASING STEP BY STEP\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    # The Policy Function\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_predictions, q_X,\n",
    "        len(poss_actions))\n",
    "    \n",
    "    # Populate the replay memory with initial experience with existing policy and network\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = preprocess(state)\n",
    "    \n",
    "    #WE STACK THE PAST 4 FRAMES AS THE STATE INPUT, and therefore copies of the first state for a new game.\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(poss_actions[action])\n",
    "        next_state = preprocess(next_state)\n",
    "        #Move the 4 frames forward in time.\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = preprocess(state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        tot_episode_rewards.append(0)\n",
    "        tot_episode_lengths.append(0)\n",
    "        q_predictions, q_train_op, q_X, q_y, q_actions, q_summaries, q_summary_writer, q_loss = q_estimator\n",
    "        target_predictions, target_train_op, target_X, target_y, target_actions,\\\n",
    "        target_summaries, t_summary_writer, target_loss = target_estimator\n",
    "        \n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        \n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "        #Stack the initial frame 4 times initially.\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "            \n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_summary_writer.add_summary(episode_summary, total_t)\n",
    "            \n",
    "            # Update the target network every N steps\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_scope, target_scope)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # Take a step\n",
    "            #Get action probabilities for the current state\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            #Pick an action using this probability distribution\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            #Get next state by stepping through env.\n",
    "            next_state, reward, done, _ = env.step(poss_actions[action])\n",
    "            next_state = preprocess(next_state)\n",
    "            #Stack new frame and pop out fourth frame.\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "            \n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "            # Update statistics\n",
    "            tot_episode_rewards[i_episode] += reward\n",
    "            tot_episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            \n",
    "            # Calculate q values and targets using the TARGET NETWORK!\n",
    "          \n",
    "            q_values_next = sess.run(target_predictions, feed_dict= {target_X: next_states_batch})\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            # Perform gradient descent update for Q network\n",
    "            states_batch = np.array(states_batch)\n",
    "\n",
    "            feed_dict = { q_X: states_batch, q_y: targets_batch, q_actions: action_batch }\n",
    "            summaries, global_step, _, loss = sess.run(\n",
    "                [q_summaries, tf.contrib.framework.get_global_step(), q_train_op, q_loss],\n",
    "            feed_dict)\n",
    "            \n",
    "            #Write summaries\n",
    "            q_summary_writer.add_summary(summaries, global_step)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "        # Add summaries to tensorboard and flush after writing\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=tot_episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=tot_episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_summary_writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function to run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-bd12c489730f>:41: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "Loading model checkpoint breakout_experiments/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from breakout_experiments/checkpoints/model\n",
      "Populating replay memory...\n",
      "Step 319 (1287999) @ Episode 10/50000, loss: 0.00492992205545306236\n",
      "Copied model parameters to target network.\n",
      "Step 374 (1295999) @ Episode 28/50000, loss: 0.00016379404405597597\n",
      "Copied model parameters to target network.\n",
      "Step 289 (1303999) @ Episode 44/50000, loss: 0.00227616727352142334\n",
      "Copied model parameters to target network.\n",
      "Step 762 (1311999) @ Episode 58/50000, loss: 0.00068362872116267683\n",
      "Copied model parameters to target network.\n",
      "Step 216 (1319999) @ Episode 72/50000, loss: 0.00080721604172140366\n",
      "Copied model parameters to target network.\n",
      "Step 708 (1327999) @ Episode 86/50000, loss: 0.00042133458191528916\n",
      "Copied model parameters to target network.\n",
      "Step 430 (1335999) @ Episode 100/50000, loss: 0.00046368606854230165\n",
      "Copied model parameters to target network.\n",
      "Step 680 (1343999) @ Episode 115/50000, loss: 0.00037102671922184527\n",
      "Copied model parameters to target network.\n",
      "Step 580 (1351999) @ Episode 129/50000, loss: 0.00033904687734320767\n",
      "Copied model parameters to target network.\n",
      "Step 243 (1359999) @ Episode 145/50000, loss: 0.00090609269682317977\n",
      "Copied model parameters to target network.\n",
      "Step 163 (1367999) @ Episode 160/50000, loss: 0.00092182098887860774\n",
      "Copied model parameters to target network.\n",
      "Step 229 (1375999) @ Episode 174/50000, loss: 0.00107586721424013385\n",
      "Copied model parameters to target network.\n",
      "Step 254 (1383999) @ Episode 188/50000, loss: 0.00109563604928553164\n",
      "Copied model parameters to target network.\n",
      "Step 103 (1391999) @ Episode 202/50000, loss: 0.00093679968267679217\n",
      "Copied model parameters to target network.\n",
      "Step 492 (1399999) @ Episode 216/50000, loss: 0.00106875598430633546\n",
      "Copied model parameters to target network.\n",
      "Step 54 (1407999) @ Episode 231/50000, loss: 0.000648620654828846576\n",
      "Copied model parameters to target network.\n",
      "Step 463 (1415999) @ Episode 245/50000, loss: 0.00063246896024793393\n",
      "Copied model parameters to target network.\n",
      "Step 360 (1423999) @ Episode 258/50000, loss: 0.00113882496953010566\n",
      "Copied model parameters to target network.\n",
      "Step 230 (1431999) @ Episode 274/50000, loss: 0.00063770165434107185\n",
      "Copied model parameters to target network.\n",
      "Step 436 (1434854) @ Episode 279/50000, loss: 0.00105116027407348163"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.relpath(\"./breakout_experiments\")\n",
    "# Create a global step variable for training iteration count\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create graphs for Q and target networks and store nodes in tuples\n",
    "q_estimator = build_model(scope=\"q\")\n",
    "target_estimator = build_model(scope=\"target_q\")\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    #Initialize graphs\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #RUN DQN\n",
    "    deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator=q_estimator,\n",
    "                    target_estimator=target_estimator,\n",
    "                    experiment_dir=experiment_dir,\n",
    "                    num_episodes=50000,\n",
    "                    replay_memory_size=800000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=8000,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    discount_factor=0.95,\n",
    "                    batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
